{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4cf112-299e-47fa-84a7-f7b00074711c",
   "metadata": {},
   "source": [
    "# Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cccf95-6b42-4d65-bf0e-a3e54a130025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888335f1-30b3-4be0-ab4b-6e91df64e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f53376-5f37-4233-aec4-4044bdae1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63f310-dbcf-4f01-b5cd-a2901de5d648",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a8d1a-d9f0-4724-8f86-abff7a9384aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "# /opt/manual/spark: this is SPARK_HOME path\n",
    "findspark.init(\"C:\\spark\\spark-3.3.2-bin-hadoop3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4760f7ff-e3d4-4445-b5c1-ff7461571867",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = \"C:\\spark\\spark-3.3.2-bin-hadoop3\\bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a51847-91fe-49a8-b39a-3d53cb0ab143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext as sc\n",
    "import spark\n",
    "import pyspark\n",
    "from pyspark import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c32db-bec9-4027-818e-dfddd9dce0fa",
   "metadata": {},
   "source": [
    "# Spark configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ac626-944b-428b-8c72-c87edce3fd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.executor.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\"). \\\n",
    "set(\"spark.driver.extraJavaOptions\",\"-Dcom.amazonaws.services.s3.enableV4=true\"). \\\n",
    "setAppName(\"pyspark_aws\").setMaster(\"local[*]\")\n",
    "\n",
    "sc=SparkContext(conf=conf)\n",
    "sc.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1691fc-77b6-4b32-b07f-8cd5f12189d4",
   "metadata": {},
   "source": [
    "# Set Spark Hadoop properties for all worker nodes as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b76c71-3755-44af-aa92-bc06f81e5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "accessKeyId=\"your_access_key\"\n",
    "secretAccessKey=\"your_secret_key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02762273-3f27-41c0-be38-18f11152dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", accessKeyId)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secretAccessKey)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa292d-9dbd-4da8-a99a-2520dfef00a7",
   "metadata": {},
   "source": [
    "# A sample of data by pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d8b34-368f-4ebb-85c0-11c9a2fdb072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"./samples/beacons.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58006bda-7a75-46fc-bd3c-40077f9e493e",
   "metadata": {},
   "source": [
    "# Copy local csv file to s3 without using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9620bb-b89c-434e-9ea6-f442ce0f245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "$ aws s3 cp ./samples/beacons.csv s3://my-bucket-123/beacons.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c6e596-bac9-4dcf-8023-37a329099605",
   "metadata": {},
   "source": [
    "# Read the dataset on local system and put in spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427b5c2-40c4-4c0c-8d25-d0f105e89d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.read.csv(\"./samples/beacons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c49871c-102f-4b7f-8594-cc827c4a2024",
   "metadata": {},
   "source": [
    "# Write Data to AWS S3 with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf4e57-9b63-4137-900b-f8f9c6917202",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.write.format(\"csv\").option(\"header\",\"true\").save(\"s3a://<your_bucket_name_here>/<your_folder_here>\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13841d6-feb9-4c54-8ee4-dee3d3b25af8",
   "metadata": {},
   "source": [
    "# Read Data from AWS S3 into PySpark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddb677-13bd-4411-8226-b33f0aeeb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s3 = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"s3a://<your_bucket_name_here>/<your_folder_here>\")\n",
    "df_s3.show(5)\n",
    "#OR\n",
    "\n",
    "df_s3 = spark.read.csv(\"s3a://your_bucket_name_here/beacons.csv\")\n",
    "df_s3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db85c56-f182-4875-9364-381baeed978b",
   "metadata": {},
   "source": [
    "### The df_s3 is a prepared dataset for passing to machine learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf74259-c72f-4dbe-aa0b-2ce6dd5234d2",
   "metadata": {},
   "source": [
    "### *** Results of machine learning algorithms are (as a CSV file) written to s3 like rdd example in the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d2427-969b-4542-bc45-94847a6d4c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
